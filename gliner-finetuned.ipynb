{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":14651969,"sourceType":"datasetVersion","datasetId":9359972}],"dockerImageVersionId":31259,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\n!pip uninstall -y transformers accelerate peft gliner numpy\n\n!pip install spacy \"transformers==4.57.6\" \"accelerate==1.12.0\" \"peft==0.18.1\" \"gliner==0.2.24\" \"numpy==2.0.2\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-29T08:53:54.256913Z","iopub.execute_input":"2026-01-29T08:53:54.257364Z","iopub.status.idle":"2026-01-29T08:54:18.205389Z","shell.execute_reply.started":"2026-01-29T08:53:54.257333Z","shell.execute_reply":"2026-01-29T08:54:18.204468Z"}},"outputs":[{"name":"stdout","text":"Found existing installation: transformers 4.57.1\nUninstalling transformers-4.57.1:\n  Successfully uninstalled transformers-4.57.1\nFound existing installation: accelerate 1.11.0\nUninstalling accelerate-1.11.0:\n  Successfully uninstalled accelerate-1.11.0\nFound existing installation: peft 0.17.1\nUninstalling peft-0.17.1:\n  Successfully uninstalled peft-0.17.1\n\u001b[33mWARNING: Skipping gliner as it is not installed.\u001b[0m\u001b[33m\n\u001b[0mFound existing installation: numpy 2.0.2\nUninstalling numpy-2.0.2:\n  Successfully uninstalled numpy-2.0.2\nRequirement already satisfied: spacy in /usr/local/lib/python3.12/dist-packages (3.8.7)\nCollecting transformers==4.57.6\n  Downloading transformers-4.57.6-py3-none-any.whl.metadata (43 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting accelerate==1.12.0\n  Downloading accelerate-1.12.0-py3-none-any.whl.metadata (19 kB)\nCollecting peft==0.18.1\n  Downloading peft-0.18.1-py3-none-any.whl.metadata (14 kB)\nCollecting gliner==0.2.24\n  Downloading gliner-0.2.24-py3-none-any.whl.metadata (10 kB)\nCollecting numpy==2.0.2\n  Downloading numpy-2.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.9/60.9 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers==4.57.6) (3.20.3)\nRequirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.57.6) (0.36.0)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.57.6) (26.0rc2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.57.6) (6.0.3)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.57.6) (2025.11.3)\nRequirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers==4.57.6) (2.32.5)\nRequirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.57.6) (0.22.1)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers==4.57.6) (0.6.2)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers==4.57.6) (4.67.1)\nRequirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate==1.12.0) (5.9.5)\nRequirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate==1.12.0) (2.8.0+cu126)\nCollecting onnxruntime (from gliner==0.2.24)\n  Downloading onnxruntime-1.23.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.1 kB)\nRequirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (from gliner==0.2.24) (0.2.1)\nRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\nRequirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.5)\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.13)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.11)\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.10)\nRequirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (8.3.6)\nRequirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.1.3)\nRequirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.5.1)\nRequirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.10)\nRequirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.4.1)\nRequirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.20.0)\nRequirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.12.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.1.6)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from spacy) (75.2.0)\nRequirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.5.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers==4.57.6) (2025.10.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers==4.57.6) (4.15.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers==4.57.6) (1.2.1rc0)\nRequirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.12/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\nRequirement already satisfied: pydantic-core==2.41.5 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.41.5)\nRequirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.2)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.57.6) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.57.6) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.57.6) (2.6.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.57.6) (2026.1.4)\nRequirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\nRequirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\nRequirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate==1.12.0) (1.13.3)\nRequirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate==1.12.0) (3.5)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate==1.12.0) (12.6.77)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate==1.12.0) (12.6.77)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate==1.12.0) (12.6.80)\nRequirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate==1.12.0) (9.10.2.21)\nRequirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate==1.12.0) (12.6.4.1)\nRequirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate==1.12.0) (11.3.0.4)\nRequirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate==1.12.0) (10.3.7.77)\nRequirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate==1.12.0) (11.7.1.2)\nRequirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate==1.12.0) (12.5.4.2)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate==1.12.0) (0.7.1)\nRequirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate==1.12.0) (2.27.3)\nRequirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate==1.12.0) (12.6.77)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate==1.12.0) (12.6.85)\nRequirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate==1.12.0) (1.11.1.6)\nRequirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate==1.12.0) (3.4.0)\nRequirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.3.1)\nRequirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\nRequirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (14.2.0)\nRequirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.23.0)\nRequirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.4.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->spacy) (3.0.3)\nCollecting coloredlogs (from onnxruntime->gliner==0.2.24)\n  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime->gliner==0.2.24) (25.9.23)\nRequirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from onnxruntime->gliner==0.2.24) (5.29.5)\nRequirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.3.1)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (4.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.2)\nRequirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (2.0.0)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate==1.12.0) (1.3.0)\nCollecting humanfriendly>=9.1 (from coloredlogs->onnxruntime->gliner==0.2.24)\n  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\nDownloading transformers-4.57.6-py3-none-any.whl (12.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m112.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:01\u001b[0m\n\u001b[?25hDownloading accelerate-1.12.0-py3-none-any.whl (380 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m380.9/380.9 kB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading peft-0.18.1-py3-none-any.whl (556 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.0/557.0 kB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading gliner-0.2.24-py3-none-any.whl (151 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m151.9/151.9 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading numpy-2.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.2/19.2 MB\u001b[0m \u001b[31m108.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading onnxruntime-1.23.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (17.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.4/17.4 MB\u001b[0m \u001b[31m106.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: numpy, humanfriendly, coloredlogs, onnxruntime, transformers, accelerate, peft, gliner\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.26.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ngoogle-colab 1.0.0 requires google-auth==2.38.0, but you have google-auth 2.47.0 which is incompatible.\ngoogle-colab 1.0.0 requires jupyter-server==2.14.0, but you have jupyter-server 2.12.5 which is incompatible.\ngoogle-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\ncudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ngradio 5.49.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.5 which is incompatible.\nbigframes 2.26.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\nfastai 2.8.4 requires fastcore<1.9,>=1.8.0, but you have fastcore 1.11.3 which is incompatible.\npylibcudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed accelerate-1.12.0 coloredlogs-15.0.1 gliner-0.2.24 humanfriendly-10.0 numpy-2.0.2 onnxruntime-1.23.2 peft-0.18.1 transformers-4.57.6\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import transformers\nimport accelerate\nimport gliner\nimport torch\nimport spacy\nimport numpy\n\nprint(f\"Transformers: {transformers.__version__}\")\nprint(f\"Accelerate: {accelerate.__version__}\")\nprint(f\"GLiNER: {gliner.__version__}\")\nprint(f\"Torch: {torch.__version__}\")\nprint(f\"Numpy: {numpy.__version__}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-29T08:54:18.207310Z","iopub.execute_input":"2026-01-29T08:54:18.207618Z","iopub.status.idle":"2026-01-29T08:54:50.319523Z","shell.execute_reply.started":"2026-01-29T08:54:18.207589Z","shell.execute_reply":"2026-01-29T08:54:50.318765Z"}},"outputs":[{"name":"stderr","text":"2026-01-29 08:54:31.408588: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1769676871.610146      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1769676871.669924      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1769676872.183899      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1769676872.183936      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1769676872.183939      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1769676872.183941      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","output_type":"stream"},{"name":"stdout","text":"Transformers: 4.57.6\nAccelerate: 1.12.0\nGLiNER: 0.2.24\nTorch: 2.8.0+cu126\nNumpy: 2.0.2\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import json\nimport random\nimport os\nimport torch\nimport spacy\nfrom gliner import GLiNER\n\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\ntry:\n    nlp = spacy.load(\"tr_core_news_tr\")\nexcept:\n    nlp = spacy.blank(\"tr\")\n\nwith open(\"/kaggle/input/dataset/supplements_spans_ents (1).json\", \"r\", encoding=\"utf-8\") as f:\n    raw_user_data = json.load(f)\n\ndef merge_labels(data):\n    mapping = {\n        \"MARKA\": \"URUN_MARKA\", \"MARKA_DİĞER\": \"URUN_MARKA\", \"ÜRÜN_DİĞER\": \"URUN_MARKA\",\n        \"İÇERİK\": \"BILESEN\", \"BİYOMOLEKÜL\": \"BILESEN\",\n        \"HASTALIK\": \"SAGLIK_DURUMU\", \"SAĞLIK_ŞİKAYETLERİ\": \"SAGLIK_DURUMU\",\n        \"ETKİ\": \"ETKI_DENEYIM\", \"YAN_ETKİ\": \"ETKI_DENEYIM\",\n        \"KULLANICI\": \"KULLANICI_BILGISI\", \"KULLANICI_DEMOGRAFİSİ\": \"KULLANICI_BILGISI\",\n        \"TAVSİYE_EDEN\": \"TAVSİYE_EDEN\",\n        \"DOZ\":\"DOZ\",\n        \"TAT_KOKU\":\"TAT_KOKU\"\n    }\n    \n    for item in data:\n        for ent in item[\"entities\"]:\n            if ent[\"label\"] in mapping:\n                ent[\"label\"] = mapping[ent[\"label\"]]\n    return data\n\nraw_user_data = merge_labels(raw_user_data)\n\nprint(\"Sade Label Listesi:\", list(set(ent['label'] for item in raw_user_data for ent in item['entities'])))\n\ndef prepare_data(raw_data):\n    final_data = []\n    for item in raw_data:\n        text = item[\"text\"]\n        doc = nlp(text)\n        converted_entities = []\n        for ent in item[\"entities\"]:\n            span = doc.char_span(ent[\"start\"], ent[\"end\"], label=ent[\"label\"], alignment_mode=\"contract\")\n            if span:\n                converted_entities.append([span.start, span.end, span.label_])\n        \n        final_data.append({\n            \"tokenized_text\": [t.text for t in doc],\n            \"ner\": converted_entities\n        })\n    return final_data\n\n\ndataset = prepare_data(raw_user_data)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-29T08:54:50.320542Z","iopub.execute_input":"2026-01-29T08:54:50.321117Z","iopub.status.idle":"2026-01-29T08:54:51.676754Z","shell.execute_reply.started":"2026-01-29T08:54:50.321090Z","shell.execute_reply":"2026-01-29T08:54:51.676135Z"}},"outputs":[{"name":"stdout","text":"Sade Label Listesi: ['ETKI_DENEYIM', 'TAVSİYE_EDEN', 'TAT_KOKU', 'BILESEN', 'URUN_MARKA', 'DOZ', 'SAGLIK_DURUMU', 'KULLANICI_BILGISI']\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"dataset[1]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-29T08:54:51.677604Z","iopub.execute_input":"2026-01-29T08:54:51.677838Z","iopub.status.idle":"2026-01-29T08:54:51.683822Z","shell.execute_reply.started":"2026-01-29T08:54:51.677815Z","shell.execute_reply":"2026-01-29T08:54:51.683115Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"{'tokenized_text': ['Diğer',\n  'demir',\n  'ilaçlarından',\n  'farklı',\n  'olarak',\n  'mideyi',\n  'rahatsız',\n  'etmiyor',\n  '.',\n  'Saç',\n  'dökülmemi',\n  'engelliyor',\n  'yıllardır',\n  'kullanıyorum',\n  '.'],\n 'ner': [[1, 3, 'URUN_MARKA'],\n  [5, 8, 'ETKI_DENEYIM'],\n  [9, 11, 'SAGLIK_DURUMU'],\n  [9, 12, 'ETKI_DENEYIM']]}"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"random.shuffle(dataset)\nsplit_point = int(len(dataset) * 0.8)\ntrain_dataset = dataset[:split_point]\ntest_val = dataset[split_point:]\n\nsplit_point_val_test = int(len(test_val) * 0.5)\neval_dataset = test_val[:split_point_val_test]\ntest_dataset = test_val[split_point_val_test:]\n\nprint(f\"Eğitim seti:   {len(train_dataset)} \")\nprint(f\"Doğrulama seti: {len(eval_dataset)} \")\nprint(f\"Final Test seti: {len(test_dataset)} \")\n\n\ndevice = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\nprint(device)\n\nmodel = GLiNER.from_pretrained(\"urchade/gliner_multi-v2.1\")\nmodel.to(device)\n\n\nmodel.train_model(\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    output_dir=\"gliner_finetuned_optimized\",\n\n    load_best_model_at_end=True,     \n    metric_for_best_model=\"eval_loss\", \n    greater_is_better=False,           \n    save_strategy=\"epoch\",            \n    eval_strategy=\"epoch\",             \n    save_total_limit=1,                \n    \n    learning_rate=5e-6,\n    others_lr=5e-5,                   \n    weight_decay=0.01,\n    others_weight_decay=0.01,\n    warmup_ratio=0.1,\n    lr_scheduler_type=\"cosine\",\n    \n    num_train_epochs=10,                \n    max_steps=-1,                      \n    logging_strategy=\"steps\",\n    logging_steps=50,\n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=4,\n    focal_loss_alpha=0.75,\n    focal_loss_gamma=2,\n    \n    dataloader_num_workers=0,\n    report_to=\"none\",\n    bf16=False, \n    fp16=False                         \n)\n\noutput_dir=\"/kaggle/working/best_model_10epoch_final\"\nmodel.save_pretrained(output_dir)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-29T08:54:51.685573Z","iopub.execute_input":"2026-01-29T08:54:51.685906Z","iopub.status.idle":"2026-01-29T09:16:29.763104Z","shell.execute_reply.started":"2026-01-29T08:54:51.685882Z","shell.execute_reply":"2026-01-29T09:16:29.762348Z"}},"outputs":[{"name":"stdout","text":"Eğitim seti:   1977 \nDoğrulama seti: 247 \nFinal Test seti: 248 \ncuda:0\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"74fd98f33f8c4a6099839f4e32c9998d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e565478302a0495da6025688f24a35ba"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":".gitattributes: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f23ea05f9da64f16bc4e8809f3bccfe8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"gliner_config.json:   0%|          | 0.00/477 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ba5dfb21200b4ea88645d0a3f960e7d8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/1.16G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7b703125d2334e82a749e55b07d4773b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.16G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"089c18f3121c4402b32347bbd177ba43"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"240db0a45b724fb6b8612f45b9a8f9df"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/579 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e86fe9159dcc40cb8695558e3a2f898e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spm.model:   0%|          | 0.00/4.31M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"73821b5711b64c5c885a424559ef4fcc"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/transformers/convert_slow_tokenizer.py:566: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/gliner/model.py:1093: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\nThe tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 2, 'bos_token_id': 1, 'pad_token_id': 0}.\nAsking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='4950' max='4950' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [4950/4950 21:17, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>10.292300</td>\n      <td>340.748077</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>8.341300</td>\n      <td>219.241592</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>6.623800</td>\n      <td>296.067444</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>5.374400</td>\n      <td>196.222900</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>5.262900</td>\n      <td>181.849319</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>5.591100</td>\n      <td>152.121292</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>5.016700</td>\n      <td>121.592613</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>5.568900</td>\n      <td>149.095306</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>4.689500</td>\n      <td>126.451416</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>4.590900</td>\n      <td>120.742393</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"There were missing keys in the checkpoint model loaded: ['model.token_rep_layer.bert_layer.model.embeddings.word_embeddings.weight', 'model.token_rep_layer.bert_layer.model.embeddings.LayerNorm.weight', 'model.token_rep_layer.bert_layer.model.embeddings.LayerNorm.bias', 'model.token_rep_layer.bert_layer.model.encoder.layer.0.attention.self.query_proj.weight', 'model.token_rep_layer.bert_layer.model.encoder.layer.0.attention.self.query_proj.bias', 'model.token_rep_layer.bert_layer.model.encoder.layer.0.attention.self.key_proj.weight', 'model.token_rep_layer.bert_layer.model.encoder.layer.0.attention.self.key_proj.bias', 'model.token_rep_layer.bert_layer.model.encoder.layer.0.attention.self.value_proj.weight', 'model.token_rep_layer.bert_layer.model.encoder.layer.0.attention.self.value_proj.bias', 'model.token_rep_layer.bert_layer.model.encoder.layer.0.attention.output.dense.weight', 'model.token_rep_layer.bert_layer.model.encoder.layer.0.attention.output.dense.bias', 'model.token_rep_layer.bert_layer.model.encoder.layer.0.attention.output.LayerNorm.weight', 'model.token_rep_layer.bert_layer.model.encoder.layer.0.attention.output.LayerNorm.bias', 'model.token_rep_layer.bert_layer.model.encoder.layer.0.intermediate.dense.weight', 'model.token_rep_layer.bert_layer.model.encoder.layer.0.intermediate.dense.bias', 'model.token_rep_layer.bert_layer.model.encoder.layer.0.output.dense.weight', 'model.token_rep_layer.bert_layer.model.encoder.layer.0.output.dense.bias', 'model.token_rep_layer.bert_layer.model.encoder.layer.0.output.LayerNorm.weight', 'model.token_rep_layer.bert_layer.model.encoder.layer.0.output.LayerNorm.bias', 'model.token_rep_layer.bert_layer.model.encoder.layer.1.attention.self.query_proj.weight', 'model.token_rep_layer.bert_layer.model.encoder.layer.1.attention.self.query_proj.bias', 'model.token_rep_layer.bert_layer.model.encoder.layer.1.attention.self.key_proj.weight', 'model.token_rep_layer.bert_layer.model.encoder.layer.1.attention.self.key_proj.bias', 'model.token_rep_layer.bert_layer.model.encoder.layer.1.attention.self.value_proj.weight', 'model.token_rep_layer.bert_layer.model.encoder.layer.1.attention.self.value_proj.bias', 'model.token_rep_layer.bert_layer.model.encoder.layer.1.attention.output.dense.weight', 'model.token_rep_layer.bert_layer.model.encoder.layer.1.attention.output.dense.bias', 'model.token_rep_layer.bert_layer.model.encoder.layer.1.attention.output.LayerNorm.weight', 'model.token_rep_layer.bert_layer.model.encoder.layer.1.attention.output.LayerNorm.bias', 'model.token_rep_layer.bert_layer.model.encoder.layer.1.intermediate.dense.weight', 'model.token_rep_layer.bert_layer.model.encoder.layer.1.intermediate.dense.bias', 'model.token_rep_layer.bert_layer.model.encoder.layer.1.output.dense.weight', 'model.token_rep_layer.bert_layer.model.encoder.layer.1.output.dense.bias', 'model.token_rep_layer.bert_layer.model.encoder.layer.1.output.LayerNorm.weight', 'model.token_rep_layer.bert_layer.model.encoder.layer.1.output.LayerNorm.bias', 'model.token_rep_layer.bert_layer.model.encoder.layer.2.attention.self.query_proj.weight', 'model.token_rep_layer.bert_layer.model.encoder.layer.2.attention.self.query_proj.bias', 'model.token_rep_layer.bert_layer.model.encoder.layer.2.attention.self.key_proj.weight', 'model.token_rep_layer.bert_layer.model.encoder.layer.2.attention.self.key_proj.bias', 'model.token_rep_layer.bert_layer.model.encoder.layer.2.attention.self.value_proj.weight', 'model.token_rep_layer.bert_layer.model.encoder.layer.2.attention.self.value_proj.bias', 'model.token_rep_layer.bert_layer.model.encoder.layer.2.attention.output.dense.weight', 'model.token_rep_layer.bert_layer.model.encoder.layer.2.attention.output.dense.bias', 'model.token_rep_layer.bert_layer.model.encoder.layer.2.attention.output.LayerNorm.weight', 'model.token_rep_layer.bert_layer.model.encoder.layer.2.attention.output.LayerNorm.bias', 'model.token_rep_layer.bert_layer.model.encoder.layer.2.intermediate.dense.weight', 'model.token_rep_layer.bert_layer.model.encoder.layer.2.intermediate.dense.bias', 'model.token_rep_layer.bert_layer.model.encoder.layer.2.output.dense.weight', 'model.token_rep_layer.bert_layer.model.encoder.layer.2.output.dense.bias', 'model.token_rep_layer.bert_layer.model.encoder.layer.2.output.LayerNorm.weight', 'model.token_rep_layer.bert_layer.model.encoder.layer.2.output.LayerNorm.bias', 'model.token_rep_layer.bert_layer.model.encoder.layer.3.attention.self.query_proj.weight', 'model.token_rep_layer.bert_layer.model.encoder.layer.3.attention.self.query_proj.bias', 'model.token_rep_layer.bert_layer.model.encoder.layer.3.attention.self.key_proj.weight', 'model.token_rep_layer.bert_layer.model.encoder.layer.3.attention.self.key_proj.bias', 'model.token_rep_layer.bert_layer.model.encoder.layer.3.attention.self.value_proj.weight', 'model.token_rep_layer.bert_layer.model.encoder.layer.3.attention.self.value_proj.bias', 'model.token_rep_layer.bert_layer.model.encoder.layer.3.attention.output.dense.weight', 'model.token_rep_layer.bert_layer.model.encoder.layer.3.attention.output.dense.bias', 'model.token_rep_layer.bert_layer.model.encoder.layer.3.attention.output.LayerNorm.weight', 'model.token_rep_layer.bert_layer.model.encoder.layer.3.attention.output.LayerNorm.bias', 'model.token_rep_layer.bert_layer.model.encoder.layer.3.intermediate.dense.weight', 'model.token_rep_layer.bert_layer.model.encoder.layer.3.intermediate.dense.bias', 'model.token_rep_layer.bert_layer.model.encoder.layer.3.output.dense.weight', 'model.token_rep_layer.bert_layer.model.encoder.layer.3.output.dense.bias', 'model.token_rep_layer.bert_layer.model.encoder.layer.3.output.LayerNorm.weight', 'model.token_rep_layer.bert_layer.model.encoder.layer.3.output.LayerNorm.bias', 'model.token_rep_layer.bert_layer.model.encoder.layer.4.attention.self.query_proj.weight', 'model.token_rep_layer.bert_layer.model.encoder.layer.4.attention.self.query_proj.bias', 'model.token_rep_layer.bert_layer.model.encoder.layer.4.attention.self.key_proj.weight', 'model.token_rep_layer.bert_layer.model.encoder.layer.4.attention.self.key_proj.bias', 'model.token_rep_layer.bert_layer.model.encoder.layer.4.attention.self.value_proj.weight', 'model.token_rep_layer.bert_layer.model.encoder.layer.4.attention.self.value_proj.bias', 'model.token_rep_layer.bert_layer.model.encoder.layer.4.attention.output.dense.weight', 'model.token_rep_layer.bert_layer.model.encoder.layer.4.attention.output.dense.bias', 'model.token_rep_layer.bert_layer.model.encoder.layer.4.attention.output.LayerNorm.weight', 'model.token_rep_layer.bert_layer.model.encoder.layer.4.attention.output.LayerNorm.bias', 'model.token_rep_layer.bert_layer.model.encoder.layer.4.intermediate.dense.weight', 'model.token_rep_layer.bert_layer.model.encoder.layer.4.intermediate.dense.bias', 'model.token_rep_layer.bert_layer.model.encoder.layer.4.output.dense.weight', 'model.token_rep_layer.bert_layer.model.encoder.layer.4.output.dense.bias', 'model.token_rep_layer.bert_layer.model.encoder.layer.4.output.LayerNorm.weight', 'model.token_rep_layer.bert_layer.model.encoder.layer.4.output.LayerNorm.bias', 'model.token_rep_layer.bert_layer.model.encoder.layer.5.attention.self.query_proj.weight', 'model.token_rep_layer.bert_layer.model.encoder.layer.5.attention.self.query_proj.bias', 'model.token_rep_layer.bert_layer.model.encoder.layer.5.attention.self.key_proj.weight', 'model.token_rep_layer.bert_layer.model.encoder.layer.5.attention.self.key_proj.bias', 'model.token_rep_layer.bert_layer.model.encoder.layer.5.attention.self.value_proj.weight', 'model.token_rep_layer.bert_layer.model.encoder.layer.5.attention.self.value_proj.bias', 'model.token_rep_layer.bert_layer.model.encoder.layer.5.attention.output.dense.weight', 'model.token_rep_layer.bert_layer.model.encoder.layer.5.attention.output.dense.bias', 'model.token_rep_layer.bert_layer.model.encoder.layer.5.attention.output.LayerNorm.weight', 'model.token_rep_layer.bert_layer.model.encoder.layer.5.attention.output.LayerNorm.bias', 'model.token_rep_layer.bert_layer.model.encoder.layer.5.intermediate.dense.weight', 'model.token_rep_layer.bert_layer.model.encoder.layer.5.intermediate.dense.bias', 'model.token_rep_layer.bert_layer.model.encoder.layer.5.output.dense.weight', 'model.token_rep_layer.bert_layer.model.encoder.layer.5.output.dense.bias', 'model.token_rep_layer.bert_layer.model.encoder.layer.5.output.LayerNorm.weight', 'model.token_rep_layer.bert_layer.model.encoder.layer.5.output.LayerNorm.bias', 'model.token_rep_layer.bert_layer.model.encoder.layer.6.attention.self.query_proj.weight', 'model.token_rep_layer.bert_layer.model.encoder.layer.6.attention.self.query_proj.bias', 'model.token_rep_layer.bert_layer.model.encoder.layer.6.attention.self.key_proj.weight', 'model.token_rep_layer.bert_layer.model.encoder.layer.6.attention.self.key_proj.bias', 'model.token_rep_layer.bert_layer.model.encoder.layer.6.attention.self.value_proj.weight', 'model.token_rep_layer.bert_layer.model.encoder.layer.6.attention.self.value_proj.bias', 'model.token_rep_layer.bert_layer.model.encoder.layer.6.attention.output.dense.weight', 'model.token_rep_layer.bert_layer.model.encoder.layer.6.attention.output.dense.bias', 'model.token_rep_layer.bert_layer.model.encoder.layer.6.attention.output.LayerNorm.weight', 'model.token_rep_layer.bert_layer.model.encoder.layer.6.attention.output.LayerNorm.bias', 'model.token_rep_layer.bert_layer.model.encoder.layer.6.intermediate.dense.weight', 'model.token_rep_layer.bert_layer.model.encoder.layer.6.intermediate.dense.bias', 'model.token_rep_layer.bert_layer.model.encoder.layer.6.output.dense.weight', 'model.token_rep_layer.bert_layer.model.encoder.layer.6.output.dense.bias', 'model.token_rep_layer.bert_layer.model.encoder.layer.6.output.LayerNorm.weight', 'model.token_rep_layer.bert_layer.model.encoder.layer.6.output.LayerNorm.bias', 'model.token_rep_layer.bert_layer.model.encoder.layer.7.attention.self.query_proj.weight', 'model.token_rep_layer.bert_layer.model.encoder.layer.7.attention.self.query_proj.bias', 'model.token_rep_layer.bert_layer.model.encoder.layer.7.attention.self.key_proj.weight', 'model.token_rep_layer.bert_layer.model.encoder.layer.7.attention.self.key_proj.bias', 'model.token_rep_layer.bert_layer.model.encoder.layer.7.attention.self.value_proj.weight', 'model.token_rep_layer.bert_layer.model.encoder.layer.7.attention.self.value_proj.bias', 'model.token_rep_layer.bert_layer.model.encoder.layer.7.attention.output.dense.weight', 'model.token_rep_layer.bert_layer.model.encoder.layer.7.attention.output.dense.bias', 'model.token_rep_layer.bert_layer.model.encoder.layer.7.attention.output.LayerNorm.weight', 'model.token_rep_layer.bert_layer.model.encoder.layer.7.attention.output.LayerNorm.bias', 'model.token_rep_layer.bert_layer.model.encoder.layer.7.intermediate.dense.weight', 'model.token_rep_layer.bert_layer.model.encoder.layer.7.intermediate.dense.bias', 'model.token_rep_layer.bert_layer.model.encoder.layer.7.output.dense.weight', 'model.token_rep_layer.bert_layer.model.encoder.layer.7.output.dense.bias', 'model.token_rep_layer.bert_layer.model.encoder.layer.7.output.LayerNorm.weight', 'model.token_rep_layer.bert_layer.model.encoder.layer.7.output.LayerNorm.bias', 'model.token_rep_layer.bert_layer.model.encoder.layer.8.attention.self.query_proj.weight', 'model.token_rep_layer.bert_layer.model.encoder.layer.8.attention.self.query_proj.bias', 'model.token_rep_layer.bert_layer.model.encoder.layer.8.attention.self.key_proj.weight', 'model.token_rep_layer.bert_layer.model.encoder.layer.8.attention.self.key_proj.bias', 'model.token_rep_layer.bert_layer.model.encoder.layer.8.attention.self.value_proj.weight', 'model.token_rep_layer.bert_layer.model.encoder.layer.8.attention.self.value_proj.bias', 'model.token_rep_layer.bert_layer.model.encoder.layer.8.attention.output.dense.weight', 'model.token_rep_layer.bert_layer.model.encoder.layer.8.attention.output.dense.bias', 'model.token_rep_layer.bert_layer.model.encoder.layer.8.attention.output.LayerNorm.weight', 'model.token_rep_layer.bert_layer.model.encoder.layer.8.attention.output.LayerNorm.bias', 'model.token_rep_layer.bert_layer.model.encoder.layer.8.intermediate.dense.weight', 'model.token_rep_layer.bert_layer.model.encoder.layer.8.intermediate.dense.bias', 'model.token_rep_layer.bert_layer.model.encoder.layer.8.output.dense.weight', 'model.token_rep_layer.bert_layer.model.encoder.layer.8.output.dense.bias', 'model.token_rep_layer.bert_layer.model.encoder.layer.8.output.LayerNorm.weight', 'model.token_rep_layer.bert_layer.model.encoder.layer.8.output.LayerNorm.bias', 'model.token_rep_layer.bert_layer.model.encoder.layer.9.attention.self.query_proj.weight', 'model.token_rep_layer.bert_layer.model.encoder.layer.9.attention.self.query_proj.bias', 'model.token_rep_layer.bert_layer.model.encoder.layer.9.attention.self.key_proj.weight', 'model.token_rep_layer.bert_layer.model.encoder.layer.9.attention.self.key_proj.bias', 'model.token_rep_layer.bert_layer.model.encoder.layer.9.attention.self.value_proj.weight', 'model.token_rep_layer.bert_layer.model.encoder.layer.9.attention.self.value_proj.bias', 'model.token_rep_layer.bert_layer.model.encoder.layer.9.attention.output.dense.weight', 'model.token_rep_layer.bert_layer.model.encoder.layer.9.attention.output.dense.bias', 'model.token_rep_layer.bert_layer.model.encoder.layer.9.attention.output.LayerNorm.weight', 'model.token_rep_layer.bert_layer.model.encoder.layer.9.attention.output.LayerNorm.bias', 'model.token_rep_layer.bert_layer.model.encoder.layer.9.intermediate.dense.weight', 'model.token_rep_layer.bert_layer.model.encoder.layer.9.intermediate.dense.bias', 'model.token_rep_layer.bert_layer.model.encoder.layer.9.output.dense.weight', 'model.token_rep_layer.bert_layer.model.encoder.layer.9.output.dense.bias', 'model.token_rep_layer.bert_layer.model.encoder.layer.9.output.LayerNorm.weight', 'model.token_rep_layer.bert_layer.model.encoder.layer.9.output.LayerNorm.bias', 'model.token_rep_layer.bert_layer.model.encoder.layer.10.attention.self.query_proj.weight', 'model.token_rep_layer.bert_layer.model.encoder.layer.10.attention.self.query_proj.bias', 'model.token_rep_layer.bert_layer.model.encoder.layer.10.attention.self.key_proj.weight', 'model.token_rep_layer.bert_layer.model.encoder.layer.10.attention.self.key_proj.bias', 'model.token_rep_layer.bert_layer.model.encoder.layer.10.attention.self.value_proj.weight', 'model.token_rep_layer.bert_layer.model.encoder.layer.10.attention.self.value_proj.bias', 'model.token_rep_layer.bert_layer.model.encoder.layer.10.attention.output.dense.weight', 'model.token_rep_layer.bert_layer.model.encoder.layer.10.attention.output.dense.bias', 'model.token_rep_layer.bert_layer.model.encoder.layer.10.attention.output.LayerNorm.weight', 'model.token_rep_layer.bert_layer.model.encoder.layer.10.attention.output.LayerNorm.bias', 'model.token_rep_layer.bert_layer.model.encoder.layer.10.intermediate.dense.weight', 'model.token_rep_layer.bert_layer.model.encoder.layer.10.intermediate.dense.bias', 'model.token_rep_layer.bert_layer.model.encoder.layer.10.output.dense.weight', 'model.token_rep_layer.bert_layer.model.encoder.layer.10.output.dense.bias', 'model.token_rep_layer.bert_layer.model.encoder.layer.10.output.LayerNorm.weight', 'model.token_rep_layer.bert_layer.model.encoder.layer.10.output.LayerNorm.bias', 'model.token_rep_layer.bert_layer.model.encoder.layer.11.attention.self.query_proj.weight', 'model.token_rep_layer.bert_layer.model.encoder.layer.11.attention.self.query_proj.bias', 'model.token_rep_layer.bert_layer.model.encoder.layer.11.attention.self.key_proj.weight', 'model.token_rep_layer.bert_layer.model.encoder.layer.11.attention.self.key_proj.bias', 'model.token_rep_layer.bert_layer.model.encoder.layer.11.attention.self.value_proj.weight', 'model.token_rep_layer.bert_layer.model.encoder.layer.11.attention.self.value_proj.bias', 'model.token_rep_layer.bert_layer.model.encoder.layer.11.attention.output.dense.weight', 'model.token_rep_layer.bert_layer.model.encoder.layer.11.attention.output.dense.bias', 'model.token_rep_layer.bert_layer.model.encoder.layer.11.attention.output.LayerNorm.weight', 'model.token_rep_layer.bert_layer.model.encoder.layer.11.attention.output.LayerNorm.bias', 'model.token_rep_layer.bert_layer.model.encoder.layer.11.intermediate.dense.weight', 'model.token_rep_layer.bert_layer.model.encoder.layer.11.intermediate.dense.bias', 'model.token_rep_layer.bert_layer.model.encoder.layer.11.output.dense.weight', 'model.token_rep_layer.bert_layer.model.encoder.layer.11.output.dense.bias', 'model.token_rep_layer.bert_layer.model.encoder.layer.11.output.LayerNorm.weight', 'model.token_rep_layer.bert_layer.model.encoder.layer.11.output.LayerNorm.bias', 'model.token_rep_layer.bert_layer.model.encoder.rel_embeddings.weight', 'model.token_rep_layer.bert_layer.model.encoder.LayerNorm.weight', 'model.token_rep_layer.bert_layer.model.encoder.LayerNorm.bias', 'model.token_rep_layer.projection.weight', 'model.token_rep_layer.projection.bias', 'model.rnn.lstm.weight_ih_l0', 'model.rnn.lstm.weight_hh_l0', 'model.rnn.lstm.bias_ih_l0', 'model.rnn.lstm.bias_hh_l0', 'model.rnn.lstm.weight_ih_l0_reverse', 'model.rnn.lstm.weight_hh_l0_reverse', 'model.rnn.lstm.bias_ih_l0_reverse', 'model.rnn.lstm.bias_hh_l0_reverse', 'model.span_rep_layer.span_rep_layer.project_start.0.weight', 'model.span_rep_layer.span_rep_layer.project_start.0.bias', 'model.span_rep_layer.span_rep_layer.project_start.3.weight', 'model.span_rep_layer.span_rep_layer.project_start.3.bias', 'model.span_rep_layer.span_rep_layer.project_end.0.weight', 'model.span_rep_layer.span_rep_layer.project_end.0.bias', 'model.span_rep_layer.span_rep_layer.project_end.3.weight', 'model.span_rep_layer.span_rep_layer.project_end.3.bias', 'model.span_rep_layer.span_rep_layer.out_project.0.weight', 'model.span_rep_layer.span_rep_layer.out_project.0.bias', 'model.span_rep_layer.span_rep_layer.out_project.3.weight', 'model.span_rep_layer.span_rep_layer.out_project.3.bias', 'model.prompt_rep_layer.0.weight', 'model.prompt_rep_layer.0.bias', 'model.prompt_rep_layer.3.weight', 'model.prompt_rep_layer.3.bias'].\nThere were unexpected keys in the checkpoint model loaded: ['token_rep_layer.bert_layer.model.embeddings.word_embeddings.weight', 'token_rep_layer.bert_layer.model.embeddings.LayerNorm.weight', 'token_rep_layer.bert_layer.model.embeddings.LayerNorm.bias', 'token_rep_layer.bert_layer.model.encoder.layer.0.attention.self.query_proj.weight', 'token_rep_layer.bert_layer.model.encoder.layer.0.attention.self.query_proj.bias', 'token_rep_layer.bert_layer.model.encoder.layer.0.attention.self.key_proj.weight', 'token_rep_layer.bert_layer.model.encoder.layer.0.attention.self.key_proj.bias', 'token_rep_layer.bert_layer.model.encoder.layer.0.attention.self.value_proj.weight', 'token_rep_layer.bert_layer.model.encoder.layer.0.attention.self.value_proj.bias', 'token_rep_layer.bert_layer.model.encoder.layer.0.attention.output.dense.weight', 'token_rep_layer.bert_layer.model.encoder.layer.0.attention.output.dense.bias', 'token_rep_layer.bert_layer.model.encoder.layer.0.attention.output.LayerNorm.weight', 'token_rep_layer.bert_layer.model.encoder.layer.0.attention.output.LayerNorm.bias', 'token_rep_layer.bert_layer.model.encoder.layer.0.intermediate.dense.weight', 'token_rep_layer.bert_layer.model.encoder.layer.0.intermediate.dense.bias', 'token_rep_layer.bert_layer.model.encoder.layer.0.output.dense.weight', 'token_rep_layer.bert_layer.model.encoder.layer.0.output.dense.bias', 'token_rep_layer.bert_layer.model.encoder.layer.0.output.LayerNorm.weight', 'token_rep_layer.bert_layer.model.encoder.layer.0.output.LayerNorm.bias', 'token_rep_layer.bert_layer.model.encoder.layer.1.attention.self.query_proj.weight', 'token_rep_layer.bert_layer.model.encoder.layer.1.attention.self.query_proj.bias', 'token_rep_layer.bert_layer.model.encoder.layer.1.attention.self.key_proj.weight', 'token_rep_layer.bert_layer.model.encoder.layer.1.attention.self.key_proj.bias', 'token_rep_layer.bert_layer.model.encoder.layer.1.attention.self.value_proj.weight', 'token_rep_layer.bert_layer.model.encoder.layer.1.attention.self.value_proj.bias', 'token_rep_layer.bert_layer.model.encoder.layer.1.attention.output.dense.weight', 'token_rep_layer.bert_layer.model.encoder.layer.1.attention.output.dense.bias', 'token_rep_layer.bert_layer.model.encoder.layer.1.attention.output.LayerNorm.weight', 'token_rep_layer.bert_layer.model.encoder.layer.1.attention.output.LayerNorm.bias', 'token_rep_layer.bert_layer.model.encoder.layer.1.intermediate.dense.weight', 'token_rep_layer.bert_layer.model.encoder.layer.1.intermediate.dense.bias', 'token_rep_layer.bert_layer.model.encoder.layer.1.output.dense.weight', 'token_rep_layer.bert_layer.model.encoder.layer.1.output.dense.bias', 'token_rep_layer.bert_layer.model.encoder.layer.1.output.LayerNorm.weight', 'token_rep_layer.bert_layer.model.encoder.layer.1.output.LayerNorm.bias', 'token_rep_layer.bert_layer.model.encoder.layer.2.attention.self.query_proj.weight', 'token_rep_layer.bert_layer.model.encoder.layer.2.attention.self.query_proj.bias', 'token_rep_layer.bert_layer.model.encoder.layer.2.attention.self.key_proj.weight', 'token_rep_layer.bert_layer.model.encoder.layer.2.attention.self.key_proj.bias', 'token_rep_layer.bert_layer.model.encoder.layer.2.attention.self.value_proj.weight', 'token_rep_layer.bert_layer.model.encoder.layer.2.attention.self.value_proj.bias', 'token_rep_layer.bert_layer.model.encoder.layer.2.attention.output.dense.weight', 'token_rep_layer.bert_layer.model.encoder.layer.2.attention.output.dense.bias', 'token_rep_layer.bert_layer.model.encoder.layer.2.attention.output.LayerNorm.weight', 'token_rep_layer.bert_layer.model.encoder.layer.2.attention.output.LayerNorm.bias', 'token_rep_layer.bert_layer.model.encoder.layer.2.intermediate.dense.weight', 'token_rep_layer.bert_layer.model.encoder.layer.2.intermediate.dense.bias', 'token_rep_layer.bert_layer.model.encoder.layer.2.output.dense.weight', 'token_rep_layer.bert_layer.model.encoder.layer.2.output.dense.bias', 'token_rep_layer.bert_layer.model.encoder.layer.2.output.LayerNorm.weight', 'token_rep_layer.bert_layer.model.encoder.layer.2.output.LayerNorm.bias', 'token_rep_layer.bert_layer.model.encoder.layer.3.attention.self.query_proj.weight', 'token_rep_layer.bert_layer.model.encoder.layer.3.attention.self.query_proj.bias', 'token_rep_layer.bert_layer.model.encoder.layer.3.attention.self.key_proj.weight', 'token_rep_layer.bert_layer.model.encoder.layer.3.attention.self.key_proj.bias', 'token_rep_layer.bert_layer.model.encoder.layer.3.attention.self.value_proj.weight', 'token_rep_layer.bert_layer.model.encoder.layer.3.attention.self.value_proj.bias', 'token_rep_layer.bert_layer.model.encoder.layer.3.attention.output.dense.weight', 'token_rep_layer.bert_layer.model.encoder.layer.3.attention.output.dense.bias', 'token_rep_layer.bert_layer.model.encoder.layer.3.attention.output.LayerNorm.weight', 'token_rep_layer.bert_layer.model.encoder.layer.3.attention.output.LayerNorm.bias', 'token_rep_layer.bert_layer.model.encoder.layer.3.intermediate.dense.weight', 'token_rep_layer.bert_layer.model.encoder.layer.3.intermediate.dense.bias', 'token_rep_layer.bert_layer.model.encoder.layer.3.output.dense.weight', 'token_rep_layer.bert_layer.model.encoder.layer.3.output.dense.bias', 'token_rep_layer.bert_layer.model.encoder.layer.3.output.LayerNorm.weight', 'token_rep_layer.bert_layer.model.encoder.layer.3.output.LayerNorm.bias', 'token_rep_layer.bert_layer.model.encoder.layer.4.attention.self.query_proj.weight', 'token_rep_layer.bert_layer.model.encoder.layer.4.attention.self.query_proj.bias', 'token_rep_layer.bert_layer.model.encoder.layer.4.attention.self.key_proj.weight', 'token_rep_layer.bert_layer.model.encoder.layer.4.attention.self.key_proj.bias', 'token_rep_layer.bert_layer.model.encoder.layer.4.attention.self.value_proj.weight', 'token_rep_layer.bert_layer.model.encoder.layer.4.attention.self.value_proj.bias', 'token_rep_layer.bert_layer.model.encoder.layer.4.attention.output.dense.weight', 'token_rep_layer.bert_layer.model.encoder.layer.4.attention.output.dense.bias', 'token_rep_layer.bert_layer.model.encoder.layer.4.attention.output.LayerNorm.weight', 'token_rep_layer.bert_layer.model.encoder.layer.4.attention.output.LayerNorm.bias', 'token_rep_layer.bert_layer.model.encoder.layer.4.intermediate.dense.weight', 'token_rep_layer.bert_layer.model.encoder.layer.4.intermediate.dense.bias', 'token_rep_layer.bert_layer.model.encoder.layer.4.output.dense.weight', 'token_rep_layer.bert_layer.model.encoder.layer.4.output.dense.bias', 'token_rep_layer.bert_layer.model.encoder.layer.4.output.LayerNorm.weight', 'token_rep_layer.bert_layer.model.encoder.layer.4.output.LayerNorm.bias', 'token_rep_layer.bert_layer.model.encoder.layer.5.attention.self.query_proj.weight', 'token_rep_layer.bert_layer.model.encoder.layer.5.attention.self.query_proj.bias', 'token_rep_layer.bert_layer.model.encoder.layer.5.attention.self.key_proj.weight', 'token_rep_layer.bert_layer.model.encoder.layer.5.attention.self.key_proj.bias', 'token_rep_layer.bert_layer.model.encoder.layer.5.attention.self.value_proj.weight', 'token_rep_layer.bert_layer.model.encoder.layer.5.attention.self.value_proj.bias', 'token_rep_layer.bert_layer.model.encoder.layer.5.attention.output.dense.weight', 'token_rep_layer.bert_layer.model.encoder.layer.5.attention.output.dense.bias', 'token_rep_layer.bert_layer.model.encoder.layer.5.attention.output.LayerNorm.weight', 'token_rep_layer.bert_layer.model.encoder.layer.5.attention.output.LayerNorm.bias', 'token_rep_layer.bert_layer.model.encoder.layer.5.intermediate.dense.weight', 'token_rep_layer.bert_layer.model.encoder.layer.5.intermediate.dense.bias', 'token_rep_layer.bert_layer.model.encoder.layer.5.output.dense.weight', 'token_rep_layer.bert_layer.model.encoder.layer.5.output.dense.bias', 'token_rep_layer.bert_layer.model.encoder.layer.5.output.LayerNorm.weight', 'token_rep_layer.bert_layer.model.encoder.layer.5.output.LayerNorm.bias', 'token_rep_layer.bert_layer.model.encoder.layer.6.attention.self.query_proj.weight', 'token_rep_layer.bert_layer.model.encoder.layer.6.attention.self.query_proj.bias', 'token_rep_layer.bert_layer.model.encoder.layer.6.attention.self.key_proj.weight', 'token_rep_layer.bert_layer.model.encoder.layer.6.attention.self.key_proj.bias', 'token_rep_layer.bert_layer.model.encoder.layer.6.attention.self.value_proj.weight', 'token_rep_layer.bert_layer.model.encoder.layer.6.attention.self.value_proj.bias', 'token_rep_layer.bert_layer.model.encoder.layer.6.attention.output.dense.weight', 'token_rep_layer.bert_layer.model.encoder.layer.6.attention.output.dense.bias', 'token_rep_layer.bert_layer.model.encoder.layer.6.attention.output.LayerNorm.weight', 'token_rep_layer.bert_layer.model.encoder.layer.6.attention.output.LayerNorm.bias', 'token_rep_layer.bert_layer.model.encoder.layer.6.intermediate.dense.weight', 'token_rep_layer.bert_layer.model.encoder.layer.6.intermediate.dense.bias', 'token_rep_layer.bert_layer.model.encoder.layer.6.output.dense.weight', 'token_rep_layer.bert_layer.model.encoder.layer.6.output.dense.bias', 'token_rep_layer.bert_layer.model.encoder.layer.6.output.LayerNorm.weight', 'token_rep_layer.bert_layer.model.encoder.layer.6.output.LayerNorm.bias', 'token_rep_layer.bert_layer.model.encoder.layer.7.attention.self.query_proj.weight', 'token_rep_layer.bert_layer.model.encoder.layer.7.attention.self.query_proj.bias', 'token_rep_layer.bert_layer.model.encoder.layer.7.attention.self.key_proj.weight', 'token_rep_layer.bert_layer.model.encoder.layer.7.attention.self.key_proj.bias', 'token_rep_layer.bert_layer.model.encoder.layer.7.attention.self.value_proj.weight', 'token_rep_layer.bert_layer.model.encoder.layer.7.attention.self.value_proj.bias', 'token_rep_layer.bert_layer.model.encoder.layer.7.attention.output.dense.weight', 'token_rep_layer.bert_layer.model.encoder.layer.7.attention.output.dense.bias', 'token_rep_layer.bert_layer.model.encoder.layer.7.attention.output.LayerNorm.weight', 'token_rep_layer.bert_layer.model.encoder.layer.7.attention.output.LayerNorm.bias', 'token_rep_layer.bert_layer.model.encoder.layer.7.intermediate.dense.weight', 'token_rep_layer.bert_layer.model.encoder.layer.7.intermediate.dense.bias', 'token_rep_layer.bert_layer.model.encoder.layer.7.output.dense.weight', 'token_rep_layer.bert_layer.model.encoder.layer.7.output.dense.bias', 'token_rep_layer.bert_layer.model.encoder.layer.7.output.LayerNorm.weight', 'token_rep_layer.bert_layer.model.encoder.layer.7.output.LayerNorm.bias', 'token_rep_layer.bert_layer.model.encoder.layer.8.attention.self.query_proj.weight', 'token_rep_layer.bert_layer.model.encoder.layer.8.attention.self.query_proj.bias', 'token_rep_layer.bert_layer.model.encoder.layer.8.attention.self.key_proj.weight', 'token_rep_layer.bert_layer.model.encoder.layer.8.attention.self.key_proj.bias', 'token_rep_layer.bert_layer.model.encoder.layer.8.attention.self.value_proj.weight', 'token_rep_layer.bert_layer.model.encoder.layer.8.attention.self.value_proj.bias', 'token_rep_layer.bert_layer.model.encoder.layer.8.attention.output.dense.weight', 'token_rep_layer.bert_layer.model.encoder.layer.8.attention.output.dense.bias', 'token_rep_layer.bert_layer.model.encoder.layer.8.attention.output.LayerNorm.weight', 'token_rep_layer.bert_layer.model.encoder.layer.8.attention.output.LayerNorm.bias', 'token_rep_layer.bert_layer.model.encoder.layer.8.intermediate.dense.weight', 'token_rep_layer.bert_layer.model.encoder.layer.8.intermediate.dense.bias', 'token_rep_layer.bert_layer.model.encoder.layer.8.output.dense.weight', 'token_rep_layer.bert_layer.model.encoder.layer.8.output.dense.bias', 'token_rep_layer.bert_layer.model.encoder.layer.8.output.LayerNorm.weight', 'token_rep_layer.bert_layer.model.encoder.layer.8.output.LayerNorm.bias', 'token_rep_layer.bert_layer.model.encoder.layer.9.attention.self.query_proj.weight', 'token_rep_layer.bert_layer.model.encoder.layer.9.attention.self.query_proj.bias', 'token_rep_layer.bert_layer.model.encoder.layer.9.attention.self.key_proj.weight', 'token_rep_layer.bert_layer.model.encoder.layer.9.attention.self.key_proj.bias', 'token_rep_layer.bert_layer.model.encoder.layer.9.attention.self.value_proj.weight', 'token_rep_layer.bert_layer.model.encoder.layer.9.attention.self.value_proj.bias', 'token_rep_layer.bert_layer.model.encoder.layer.9.attention.output.dense.weight', 'token_rep_layer.bert_layer.model.encoder.layer.9.attention.output.dense.bias', 'token_rep_layer.bert_layer.model.encoder.layer.9.attention.output.LayerNorm.weight', 'token_rep_layer.bert_layer.model.encoder.layer.9.attention.output.LayerNorm.bias', 'token_rep_layer.bert_layer.model.encoder.layer.9.intermediate.dense.weight', 'token_rep_layer.bert_layer.model.encoder.layer.9.intermediate.dense.bias', 'token_rep_layer.bert_layer.model.encoder.layer.9.output.dense.weight', 'token_rep_layer.bert_layer.model.encoder.layer.9.output.dense.bias', 'token_rep_layer.bert_layer.model.encoder.layer.9.output.LayerNorm.weight', 'token_rep_layer.bert_layer.model.encoder.layer.9.output.LayerNorm.bias', 'token_rep_layer.bert_layer.model.encoder.layer.10.attention.self.query_proj.weight', 'token_rep_layer.bert_layer.model.encoder.layer.10.attention.self.query_proj.bias', 'token_rep_layer.bert_layer.model.encoder.layer.10.attention.self.key_proj.weight', 'token_rep_layer.bert_layer.model.encoder.layer.10.attention.self.key_proj.bias', 'token_rep_layer.bert_layer.model.encoder.layer.10.attention.self.value_proj.weight', 'token_rep_layer.bert_layer.model.encoder.layer.10.attention.self.value_proj.bias', 'token_rep_layer.bert_layer.model.encoder.layer.10.attention.output.dense.weight', 'token_rep_layer.bert_layer.model.encoder.layer.10.attention.output.dense.bias', 'token_rep_layer.bert_layer.model.encoder.layer.10.attention.output.LayerNorm.weight', 'token_rep_layer.bert_layer.model.encoder.layer.10.attention.output.LayerNorm.bias', 'token_rep_layer.bert_layer.model.encoder.layer.10.intermediate.dense.weight', 'token_rep_layer.bert_layer.model.encoder.layer.10.intermediate.dense.bias', 'token_rep_layer.bert_layer.model.encoder.layer.10.output.dense.weight', 'token_rep_layer.bert_layer.model.encoder.layer.10.output.dense.bias', 'token_rep_layer.bert_layer.model.encoder.layer.10.output.LayerNorm.weight', 'token_rep_layer.bert_layer.model.encoder.layer.10.output.LayerNorm.bias', 'token_rep_layer.bert_layer.model.encoder.layer.11.attention.self.query_proj.weight', 'token_rep_layer.bert_layer.model.encoder.layer.11.attention.self.query_proj.bias', 'token_rep_layer.bert_layer.model.encoder.layer.11.attention.self.key_proj.weight', 'token_rep_layer.bert_layer.model.encoder.layer.11.attention.self.key_proj.bias', 'token_rep_layer.bert_layer.model.encoder.layer.11.attention.self.value_proj.weight', 'token_rep_layer.bert_layer.model.encoder.layer.11.attention.self.value_proj.bias', 'token_rep_layer.bert_layer.model.encoder.layer.11.attention.output.dense.weight', 'token_rep_layer.bert_layer.model.encoder.layer.11.attention.output.dense.bias', 'token_rep_layer.bert_layer.model.encoder.layer.11.attention.output.LayerNorm.weight', 'token_rep_layer.bert_layer.model.encoder.layer.11.attention.output.LayerNorm.bias', 'token_rep_layer.bert_layer.model.encoder.layer.11.intermediate.dense.weight', 'token_rep_layer.bert_layer.model.encoder.layer.11.intermediate.dense.bias', 'token_rep_layer.bert_layer.model.encoder.layer.11.output.dense.weight', 'token_rep_layer.bert_layer.model.encoder.layer.11.output.dense.bias', 'token_rep_layer.bert_layer.model.encoder.layer.11.output.LayerNorm.weight', 'token_rep_layer.bert_layer.model.encoder.layer.11.output.LayerNorm.bias', 'token_rep_layer.bert_layer.model.encoder.rel_embeddings.weight', 'token_rep_layer.bert_layer.model.encoder.LayerNorm.weight', 'token_rep_layer.bert_layer.model.encoder.LayerNorm.bias', 'token_rep_layer.projection.weight', 'token_rep_layer.projection.bias', 'rnn.lstm.weight_ih_l0', 'rnn.lstm.weight_hh_l0', 'rnn.lstm.bias_ih_l0', 'rnn.lstm.bias_hh_l0', 'rnn.lstm.weight_ih_l0_reverse', 'rnn.lstm.weight_hh_l0_reverse', 'rnn.lstm.bias_ih_l0_reverse', 'rnn.lstm.bias_hh_l0_reverse', 'span_rep_layer.span_rep_layer.project_start.0.weight', 'span_rep_layer.span_rep_layer.project_start.0.bias', 'span_rep_layer.span_rep_layer.project_start.3.weight', 'span_rep_layer.span_rep_layer.project_start.3.bias', 'span_rep_layer.span_rep_layer.project_end.0.weight', 'span_rep_layer.span_rep_layer.project_end.0.bias', 'span_rep_layer.span_rep_layer.project_end.3.weight', 'span_rep_layer.span_rep_layer.project_end.3.bias', 'span_rep_layer.span_rep_layer.out_project.0.weight', 'span_rep_layer.span_rep_layer.out_project.0.bias', 'span_rep_layer.span_rep_layer.out_project.3.weight', 'span_rep_layer.span_rep_layer.out_project.3.bias', 'prompt_rep_layer.0.weight', 'prompt_rep_layer.0.bias', 'prompt_rep_layer.3.weight', 'prompt_rep_layer.3.bias'].\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"all_labels = set()\n\nfor item in raw_user_data:\n    for ent in item[\"entities\"]:\n        all_labels.add(ent[\"label\"])\n\nlabels_list = list(all_labels)\n\nprint(f\"tüm etiketler ({len(labels_list)})\")\nprint(labels_list)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-29T09:16:29.764077Z","iopub.execute_input":"2026-01-29T09:16:29.764351Z","iopub.status.idle":"2026-01-29T09:16:29.771125Z","shell.execute_reply.started":"2026-01-29T09:16:29.764315Z","shell.execute_reply":"2026-01-29T09:16:29.770419Z"}},"outputs":[{"name":"stdout","text":"tüm etiketler (8)\n['ETKI_DENEYIM', 'TAVSİYE_EDEN', 'TAT_KOKU', 'BILESEN', 'URUN_MARKA', 'DOZ', 'SAGLIK_DURUMU', 'KULLANICI_BILGISI']\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"\ntest_dataset[1]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-29T09:16:29.771932Z","iopub.execute_input":"2026-01-29T09:16:29.772200Z","iopub.status.idle":"2026-01-29T09:16:41.484323Z","shell.execute_reply.started":"2026-01-29T09:16:29.772177Z","shell.execute_reply":"2026-01-29T09:16:41.483417Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"{'tokenized_text': ['Benim',\n  'iştahımı',\n  'azalttı',\n  ',',\n  'tadı',\n  'yok',\n  'bence',\n  '.',\n  'Ancak',\n  'eminim',\n  'bende',\n  'baş',\n  'ağrısı',\n  'yapıyor',\n  '.',\n  'Bu',\n  'sebeple',\n  'devam',\n  'edip',\n  'etmemeye',\n  'karar',\n  'veremedim',\n  'henüz',\n  '.'],\n 'ner': [[4, 6, 'TAT_KOKU'], [11, 13, 'ETKI_DENEYIM'], [1, 3, 'ETKI_DENEYIM']]}"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"import random\n\nmodel_gliner = GLiNER.from_pretrained(output_dir)\n\nsamples = random.sample(list(test_dataset), 10)\n\nprint(\"-\" * 70)\nprint(f\"{'TEST':^70}\")\nprint(\"-\" * 70)\n\nfor i, sample in enumerate(samples, 1):\n\n    text = \" \".join(sample[\"tokenized_text\"])\n    \n    print(f\"Test {i}: {text}\")\n    \n    print(\"\\n[ORİJİNAL VARLIKLAR ]\")\n    original_entities = []\n    for start, end, label in sample['ner']:\n        extracted_words = sample['tokenized_text'][start:end]\n        original_phrase = \" \".join(extracted_words)\n        original_entities.append(f\"{original_phrase} => {label}\")\n    \n    if not original_entities:\n        print(\"   Orijinal veride etiket bulunmuyor.\")\n    else:\n        for ent in original_entities:\n            print(f\"    {ent}\")\n\n    print(\"\\n[MODEL TAHMİNİ]\")\n    entities = model_gliner.predict_entities(text, labels_list, threshold=0.4)\n    \n    if not entities:\n        print(\"    Herhangi bir entity tespit edilemedi.\")\n    else:\n        for ent in entities:\n            print(f\"     {ent['text']} => {ent['label']} (Güven: {ent['score']:.2f})\")\n\n    print(\"\\n\" + \"-\" * 70)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-29T09:16:41.485440Z","iopub.execute_input":"2026-01-29T09:16:41.485765Z","iopub.status.idle":"2026-01-29T09:16:52.796177Z","shell.execute_reply.started":"2026-01-29T09:16:41.485737Z","shell.execute_reply":"2026-01-29T09:16:52.795191Z"}},"outputs":[{"name":"stderr","text":"Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n","output_type":"stream"},{"name":"stdout","text":"----------------------------------------------------------------------\n                                 TEST                                 \n----------------------------------------------------------------------\nTest 1: İki aldır kullanıyorum fakat saç dökülmem inanılmaz arttı nedeni nedir bilmiyorum şimdilik hiç faydasını göremedim fakat kullanmayı da bırakmadım devam ediyorum bakalım ne olacak\n\n[ORİJİNAL VARLIKLAR ]\n    İki aldır kullanıyorum fakat saç dökülmem inanılmaz arttı => ETKI_DENEYIM\n    saç dökülmem => SAGLIK_DURUMU\n\n[MODEL TAHMİNİ]\n     İki aldır kullanıyorum => DOZ (Güven: 0.79)\n     saç dökülmem inanılmaz => SAGLIK_DURUMU (Güven: 0.85)\n     şimdilik hiç faydasını göremedim fakat => ETKI_DENEYIM (Güven: 0.41)\n\n----------------------------------------------------------------------\nTest 2: ürünü geçen yıl ramazan ayında da kullanmıştım ( düzenli olarak değil tabi kaçırdığım unuttuğum zamanlar oldu)aslında tek öğün beslenme için takviye olarak tüketmiştim tabi yanında b12 ve d vitamini de almıştım hepsi de nutraxin marka idi araştırmalarım sonucu helal gıda olduğunu da öğrendiğimde hemen alıp kullanmaya başladım . tek öğün beslenme ile bana çok faydası olmuştu aman aman bir kilo problemim olmasa da kilo almaya başladığım için   bir araştırma içerisinde girdim arada kaçamaklarım oldu tabi ama o dönemde bir ay içerisinde 9 kilo vermiştim zaten 10 kilo kadar fazlam vardı . ama magnezyum sayesinde hiç açlık halsizlik yaşamadım ve gece oluşan kramplarımda tamamen ortadan kalktı . sabah yaşadığım baş ağrılarım magnezyum sayesinde hiç olmadı . yıl içerisinde de yaptırdığımız kan tahlilleri m'de gayet normal düzeyde çıktı ki benim hep eksiğim olurdu d vitamini magnezyum demir vs ama hepsi de normal çıktı ve bu ramazan ayında da sonlarına gelmiş olmamıza rağmen tekrar sipariş verdim geçen yıl verdiğim kiloyu ara ara bi kaç kilo alıp vermekle birlikte korudum . tabi bunun yanında bol su ve bitki çayı vazgeçilmezim . 2 yıldır tuttuğum oruçlar dan haz almaya başladım . ekmek makarna pilav alışkanlığım olmadığı için de magnezyum oranım gayet normal oranda ama ramazan'da çok yi yemediğim için takviye olarak aldım . satıcıya burdan teşekkür ederim çok hızlı ve korunaklı olarak elime ulaştı . paketi merak edenler içinse kendi kutusunda defalarca pat pata sarılmış ve ürün kapağı açıldığında beyaz koruyucu yapışkan kağıdı ve içerisindeki ürünlerin kırılmaması içinde sanırım pamuk vardı . tekrar teşekkürler . ürüne ve satıcıya güven 10 yıldız 🤗 💐 ara ara siparişlerimi tekrarlıycam kesin ..\n\n[ORİJİNAL VARLIKLAR ]\n    d vitamini => BILESEN\n    sabah yaşadığım baş ağrılarım => SAGLIK_DURUMU\n    benim hep eksiğim olurdu d vitamini magnezyum demir => SAGLIK_DURUMU\n    b12 => BILESEN\n    nutraxin => URUN_MARKA\n    kilo almaya başladığım => ETKI_DENEYIM\n    benim hep eksiğim olurdu d vitamini magnezyum demir vs ama hepsi de normal çıktı => ETKI_DENEYIM\n    magnezyum => BILESEN\n    magnezyum => BILESEN\n    kan tahlilleri m'de gayet normal düzeyde çıktı => ETKI_DENEYIM\n    bol su ve bitki => URUN_MARKA\n    gece oluşan kramplarımda tamamen ortadan kalktı => ETKI_DENEYIM\n\n[MODEL TAHMİNİ]\n     tek öğün beslenme => DOZ (Güven: 0.52)\n     b12 ve => BILESEN (Güven: 0.81)\n     d vitamini de => BILESEN (Güven: 0.81)\n     nutraxin marka => URUN_MARKA (Güven: 0.84)\n     tek öğün beslenme => DOZ (Güven: 0.48)\n     magnezyum sayesinde => BILESEN (Güven: 0.76)\n     açlık halsizlik yaşamadım => SAGLIK_DURUMU (Güven: 0.43)\n     magnezyum sayesinde => BILESEN (Güven: 0.73)\n     d vitamini magnezyum => BILESEN (Güven: 0.74)\n     demir vs => BILESEN (Güven: 0.79)\n     bitki çayı vazgeçilmezim => URUN_MARKA (Güven: 0.42)\n     magnezyum oranım => BILESEN (Güven: 0.75)\n\n----------------------------------------------------------------------\nTest 3: Çok hızlı teslim edildi ve 2 haftada saç dökülmemi yüzde seksen azalttı tırnaklarım da daha sağlıklı uzuyor\n\n[ORİJİNAL VARLIKLAR ]\n    saç dökülmemi yüzde seksen azalttı tırnaklarım da daha sağlıklı uzuyor => ETKI_DENEYIM\n    saç dökülmemi => SAGLIK_DURUMU\n\n[MODEL TAHMİNİ]\n     saç dökülmemi yüzde => SAGLIK_DURUMU (Güven: 0.88)\n\n----------------------------------------------------------------------\nTest 4: emzirme döneminde yorgunluk halsizlikten yattığım yerden kalkamaz hale gelmiştim ilaç gerçekten çok iyi geldi ayağa kaldırdı beni teşekkürler 👏\n\n[ORİJİNAL VARLIKLAR ]\n    emzirme döneminde => KULLANICI_BILGISI\n    yorgunluk halsizlikten yattığım yerden kalkamaz hale gelmiştim => SAGLIK_DURUMU\n    ilaç gerçekten çok iyi geldi ayağa kaldırdı => ETKI_DENEYIM\n    beni => KULLANICI_BILGISI\n\n[MODEL TAHMİNİ]\n     emzirme döneminde yorgunluk halsizlikten yattığım yerden kalkamaz hale gelmiştim ilaç => SAGLIK_DURUMU (Güven: 0.76)\n     gerçekten çok iyi geldi ayağa kaldırdı beni teşekkürler => ETKI_DENEYIM (Güven: 0.48)\n\n----------------------------------------------------------------------\nTest 5: Başka markaların biotinlerini de kullanmış biri alarak bu markayı bu ürünü daha doğrusu yetersiz buldum . Düzenli kullanmama rağmen saç dökülmelerim aynı bollukta devam etti .\n\n[ORİJİNAL VARLIKLAR ]\n    saç dökülmelerim aynı bollukta devam etti => ETKI_DENEYIM\n    biotinlerini => BILESEN\n    saç dökülmelerim => SAGLIK_DURUMU\n\n[MODEL TAHMİNİ]\n     biotinlerini de => BILESEN (Güven: 0.75)\n     biri alarak => KULLANICI_BILGISI (Güven: 0.59)\n     saç dökülmelerim aynı => SAGLIK_DURUMU (Güven: 0.83)\n\n----------------------------------------------------------------------\nTest 6: hafif migde bulantısı yapıyor saclarım daha parlak ama dökülme de pek fark göremedim henüz 1 haftadır kullanıyorumm yorumu güncellicem , güncelleme 1 hafta oldu yatmadan önce 2 tane içiyorum 2.günden beri saç dökülmem durdu ve saclarım daha parlak işe yarıyorrr\n\n[ORİJİNAL VARLIKLAR ]\n    2 tane => DOZ\n    hafif migde bulantısı yapıyor => ETKI_DENEYIM\n    saclarım daha parlak ama dökülme de pek fark göremedim => ETKI_DENEYIM\n    saç dökülmem durdu ve saclarım daha parlak => ETKI_DENEYIM\n\n[MODEL TAHMİNİ]\n     migde bulantısı yapıyor => SAGLIK_DURUMU (Güven: 0.65)\n     saclarım daha parlak ama dökülme de pek fark göremedim henüz => ETKI_DENEYIM (Güven: 0.59)\n     2 tane içiyorum => DOZ (Güven: 0.85)\n     saç dökülmem durdu => SAGLIK_DURUMU (Güven: 0.74)\n\n----------------------------------------------------------------------\nTest 7: Ürünü 1 haftadır deniyorum iştahım açıldı kendimi yorgun hissederken şimdi daha enerjik olmaya başladım denemenizde fayda var tavsiye ederim .\n\n[ORİJİNAL VARLIKLAR ]\n    iştahım açıldı => ETKI_DENEYIM\n    kendimi yorgun hissederken => SAGLIK_DURUMU\n    enerjik olmaya başladım => ETKI_DENEYIM\n\n[MODEL TAHMİNİ]\n     iştahım açıldı kendimi yorgun hissederken şimdi daha enerjik olmaya başladım denemenizde => ETKI_DENEYIM (Güven: 0.73)\n\n----------------------------------------------------------------------\nTest 8: kargo , covid yüzünden geç gelse de paketleme başarılı . 2021 in 8. ayı son kullanma tarihli . hemen alır almaz yorum yazmadım . 10 gündür kullanıyorum ve faydalarını gördüm . böyle çok enerji doldum yerim de duramıyorum diyemem fakat eskisi gibi yorgun hissetmiyorum ve kullandigimdan beri baş agrim hiç olmadı faydası var mı sebebi bu takviyemi emin değilim tabi . indirimi kacirmamak için tekrar sipariş verdim ve bu kez 2022 son kullanma tarihli geldi ki benim için süper oldu çünkü yedek aldım . paketleme yine güzel dış gorusunusu biraz daha farklı fakat içerik aynı . emeğinize sağlık\n\n[ORİJİNAL VARLIKLAR ]\n    covid => SAGLIK_DURUMU\n    böyle çok enerji doldum yerim de duramıyorum diyemem fakat eskisi gibi yorgun hissetmiyorum ve kullandigimdan beri baş agrim hiç olmadı => ETKI_DENEYIM\n    baş agrim => SAGLIK_DURUMU\n\n[MODEL TAHMİNİ]\n     covid yüzünden => SAGLIK_DURUMU (Güven: 0.76)\n     baş agrim hiç => SAGLIK_DURUMU (Güven: 0.61)\n     benim için => KULLANICI_BILGISI (Güven: 0.40)\n\n----------------------------------------------------------------------\nTest 9: Yaklaşık yirmi gündür kullanıyorum , eskiye oranla saç dökülmem azaldı ve cildimin daha canlı olduğunu düşünüyorum .\n\n[ORİJİNAL VARLIKLAR ]\n    saç dökülmem azaldı ve cildimin daha canlı olduğunu düşünüyorum => ETKI_DENEYIM\n    saç dökülmem => SAGLIK_DURUMU\n\n[MODEL TAHMİNİ]\n     saç dökülmem azaldı => SAGLIK_DURUMU (Güven: 0.87)\n\n----------------------------------------------------------------------\nTest 10: Son zamanlarda iyi uyuyamıyorum , sebebi bu mu bilmiyorum ama eskiye göre daha enerjik olduğumu gözlemledim . Sebebi bu mu bilmiyorum . İlk defa demir vitamini kullanıyorum , kullanım sonrası ağzımda demir tadı oluyor , acayip bir durum ! 😅\n\n[ORİJİNAL VARLIKLAR ]\n    kullanım sonrası ağzımda demir tadı oluyor => TAT_KOKU\n    demir vitamini => URUN_MARKA\n    iyi uyuyamıyorum , sebebi bu mu bilmiyorum => ETKI_DENEYIM\n    eskiye göre daha enerjik olduğumu gözlemledim => ETKI_DENEYIM\n\n[MODEL TAHMİNİ]\n     Son zamanlarda iyi uyuyamıyorum , => SAGLIK_DURUMU (Güven: 0.80)\n     eskiye göre daha enerjik olduğumu gözlemledim . => ETKI_DENEYIM (Güven: 0.73)\n     demir vitamini kullanıyorum => URUN_MARKA (Güven: 0.75)\n     ağzımda demir tadı oluyor , => TAT_KOKU (Güven: 0.63)\n\n----------------------------------------------------------------------\n","output_type":"stream"}],"execution_count":8}]}